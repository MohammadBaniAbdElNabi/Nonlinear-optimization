#  **Newton’s Method**

A criticism that could be made of the gradient search method is that, although the gradient direction is the best direction to move, viewed from the current trial solution, as soon as we begin moving away from the current point, that direction is immediately not the best direction any longer. And the farther we move, the worse the chosen direction becomes. The gradient direction is an especially poor choice in the neighborhood of the optimum. Therefore, convergence is not easily predictable.<br>
This behavior is explained by the fact that the gradient search method follows the gradient direction dictated by a linear approximation to $f(x)$ near the current trial solution $x^j$. Whereas a *straight line* is generally a poor approximation to a nonlinear function, most general nonlinear functions can be reasonably well approximated by a quadratic function, in the vicinity of the maximum.<br>
**Newton’s method** is an iterative technique that makes use of this fact by choosing as its next trial solution that point that maximizes the quadratic approximation to $f(x)$. Specifically, given a current trial solution $x^j$, the next point $x^{j+1}$ is computed as 

$$x^{j+1} = x^j + d^j(-H^{-1}(x^j) \bullet \nabla f(x^j))$$

where:
* $H(x)$ is the Hessian matrix of $f(x)$ evaluated at point $x$ <br> 
* $H^{-1}(x)$ is its inverse <br> 

The optimizing distance $d^j$ can be chosen just as it was in the gradient search. Convergence occurs when the direction vector becomes close to zero.<br>
**Newton’s method** generally requires fewer iterations for convergence than the gradient search method because it uses a better direction of movement from one point to the next. However, there is little else to recommend this method from a practical standpoint. First, of course, the function $f$ must be twice continuously differentiable, and the Hessian matrix must be nonsingular. The computational effort associated with inverting the Hessian matrix is excessive. (For economy of computation, it is reasonable to use the same inverse for several consecutive iterations. This slows convergence, but simplifies each iteration so much that overall performance is actually improved.)<br>
Even so, the calculations are more extensive than for the gradient search method, and the efficiency diminishes rapidly as the number of variables increases because the matrix H becomes quite large. Moreover, Newton’s method may fail to converge in general. The formula for computing a new point x$^{j+1}$ from $x^j$ does not necessarily imply an increase in the function value, for it could be that $f(x^{j+1}) < f(x^j)$. In particular, if the Hessian is positive definite, Newton’s method will approximate a quadratic minimum. If it is negative definite, it approximates a quadratic maximum. When the Hessian is indefinite, Newton’s method takes us to a saddle point solution of the approximation. Certainly, if $f(x)$ were quadratic and $H(x)$ were negative definite, then the method would converge in one iteration. In general, convergence to a local maximum is guaranteed, and occurs quite rapidly for any smooth, continuous nonlinear function once we get *close enough* to the maximum. However, *close enough* can be a very small region.