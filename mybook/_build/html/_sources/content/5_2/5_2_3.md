#  **Newtonâ€™s Method**

A criticism of the gradient search method is that while the gradient direction is initially the best direction to move from the current trial solution, it immediately ceases to be the best direction as movement begins. The further one moves, the worse the chosen direction becomes. The gradient direction is particularly ineffective near the optimum, which makes convergence difficult to predict.

This behavior stems from the fact that the gradient search method relies on a linear approximation of $f(x)$ near the current trial solution $x^j$. While a straight line is generally a poor approximation for a nonlinear function, most general nonlinear functions can be reasonably well approximated by a quadratic function in the vicinity of the maximum.

Newton's method is an iterative technique that leverages this fact by choosing as its next trial solution the point that maximizes the quadratic approximation to $f(x)$. Specifically, given a current trial solution $x^j$, the next point $x^{j+1}$ is computed as:

$$x^{j+1} = x^j + d^j(-H^{-1}(x^j) \bullet \nabla f(x^j))$$

where:
* $H(x)$ is the Hessian matrix of $f(x)$ evaluated at point $x$ <br> 
* $H^{-1}(x)$ is its inverse <br> 

The optimizing distance $d^j$ can be chosen just as it was in the gradient search. Convergence occurs when the direction vector becomes close to zero.

Newton's method generally requires fewer iterations for convergence than the gradient search method because it uses a better direction of movement from one point to the next. However, there is little else to recommend this method from a practical standpoint. First, the function $f$ must be twice continuously differentiable, and the Hessian matrix must be nonsingular. The computational effort associated with inversing the Hessian matrix is excessive. For economy of computation, it is reasonable to use the same inverse for several consecutive iterations, which slows convergence but simplifies each iteration so much that overall performance is actually improved.

Even so, the calculations are more extensive than for the gradient search method, and the efficiency diminishes rapidly as the number of variables increases because the matrix $H$ becomes quite large. Moreover, Newton's method may fail to converge in general. The formula for computing a new point $x^{j+1}$ from $x^j$ does not necessarily imply an increase in the function value, for it could be that $f(x^{j+1}) < f(x^j)$. In particular, if the Hessian is positive definite, Newton's method will approximate a quadratic minimum. If it is negative definite, it approximates a quadratic maximum. When the Hessian is indefinite, Newton's method takes us to a saddle point solution of the approximation. Certainly, if $f(x)$ were quadratic and $H(x)$ were negative definite, then the method would converge in one iteration. In general, convergence to a local maximum is guaranteed, and occurs quite rapidly for any smooth, continuous nonlinear function once we get *close enough* to the maximum. However, *close enough* can be a very small region.