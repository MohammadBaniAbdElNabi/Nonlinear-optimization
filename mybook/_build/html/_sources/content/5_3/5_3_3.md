#  Quadratic Programming

**Quadratic programming** comprises an area of mathematical programming that is second only to linear programming in its broad applicability within the field of Operations Research. While quadratic objective functions are not as simple to work with as lin- ear objectives, we can see that the gradient of a quadratic function is a *linear* function.<br> 
Consequently, the Karush-Kuhn-Tucker conditions for a quadratic programming problem have a simple form that can make solutions to these problems considerably easier to obtain than for general nonlinear programming problems.

The quadratic programming problem can be expressed in the following form:

$$\text{maximize} \quad z = \sum_{j=1}^{n} c_j x_j + \sum_{j=1}^{n} \sum_{k=1}^{n} d_{jk} x_j x_k$$

$$\text{subject to} \quad \sum_{j=1}^{n} a_{ij} x_j = b_i \text{ for i = 1, ..., m}$$

$$x_j \geq 0 \text{ for j = 1, ..., n}$$

The problem can be expressed more succinctly, using matrix notation, as:

$$\text{maximize} \quad z = c^T x + x^T D x$$
$$\text{subject to} \quad Ax = b$$
$$\quad x \geq 0$$

where:

<ul />

$x$ and $c$ are $n$-component vectors<br> 
$A$ is an $m$ x $n$ matrix <br> 
$b$ is $m$ x $1$ <br> 
$D$ is an $n$ x $n$ symmetric matrix. <br> 

</ul>

Several algorithms have been developed to solve certain forms of quadratic functions, and we will describe some of the best known and most widely used ones. Because of the complexity of these procedures, we will give only brief overviews. The curious reader is urged to consult a more advanced reference such as Simmons (1975) or Nash and Sofer (1996) for a deeper appreciation of these methods.

One of the earliest and simplest methods for solving quadratic programs is Wolfe's algo- rithm (Wolfe 1959), which is still widely used today. In this method, a sequence of feasible points is generated via a modified Simplex pivoting procedure that terminates at a point $x*$ where the Karush-Kuhn-Tucker conditions are satisfied. because the Karush-Kuhn- Tucker conditions represent a system of linear equations when the objective function is quadratic, the problem reduces to finding a feasible solution to a system of equations. Wolfe's algorithm uses phase 1 of the Simplex algorithm to find a feasible solution. The complementary slackness conditions are not linear, but the algorithm simply maintains a set of active constraints, and allows only the corresponding $\lambda_i$ dual variables to be greater than zero. Wolfe's method, like most of the later procedures, moves along an active constraint set.

When $D$ is negative definite, Wolfe's algorithm converges to an optimal solution, or demonstrates infeasibility within a finite number of iterations, assuming that the possibility of infinite cycling due to degeneracy is excluded.

Beale’s method (Beale 1959), introduced by E.M.L. Beale as early as 1955, is based on classical calculus rather than on the Karush–Kuhn–Tucker conditions. This method is applicable to any quadratic program of the form described earlier except that Beale does not require $D$ to be negative definite or negative semidefinite (i.e., the objective function need not be concave). Thus, this algorithm will generally yield local optima and the first solution generated will be the global optimum when the objective is concave.<br>
Beale’s method partitions matrices and uses partial derivatives to choose pivots until it is no longer possible to improve the objective value by any permitted change in a non-basic variable. Initially, all redundant constraints are eliminated and an initial basic feasible solution is determined via a Phase 1 Simplex process. The matrices are partitioned in such a way that a new system of equations is developed in which the basic variables, along with the associated constraints, are separated from the non-basic variables and their associated constraints. Partial derivatives determine which non-basic variable to increase or decrease.<br>
When an apparent solution is achieved, an examination of the second partial derivative will determine whether the solution is a false optimum or not. If the second partial derivative is positive for some $x$, then the current solution is a minimum (rather than a maximum). In this case, the objective function can be improved by bringing $x$ into the basis.<br>
A slightly less popular, but more recent and more sophisticated method was originally presented by Lemke (1962). It is applicable to any quadratic problem, but is typically described in terms of solving problems expressed in the form

$$ \text{maximize } z = c^T x - \frac{1}{2} x^T D x$$
$$\text{subject to } Ax \leq b$$

where $D$ is not only symmetric but also positive definite. This new restriction on $D$ is critical and is used throughout the procedure. Lemke’s formulation of the constraints in terms of inequalities rather than equations causes the Karush–Kuhn–Tucker conditions to assume a particularly simple form which is exploited by the algorithm. These constraints also include any non-negativity restrictions.<br>
Lemke’s algorithm first formulates the Karush–Kuhn–Tucker conditions for the original problem, then defines a new set of variables, from which a second quadratic program is constructed. This new problem is solved and from its solution is obtained a solution to the original problem. The basic strategy is to generate a sequence of feasible points until a point is reached at which a certain gradient satisfies two specific restrictions. Three situations may arise, each of which is handled differently but results in a matrix being updated via the usual Simplex transformation technique. When the algorithm terminates with an optimal solution to the second quadratic program, the optimal solution to the original quadratic program is constructed based on the definitions of the new set of variables.<br>
Historically, we find that Beale’s method is used less extensively in practice than the other two algorithms mentioned here. Computational experiments (Ravindran and Lee 1981) have shown that Lemke’s algorithm outperforms Wolfe’s and four other lesserknown algorithms. Although Lemke’s algorithm can fail to converge, when convergence does occur, it occurs more quickly than in the other methods.<br>
Quadratic programming models are important for a number of reasons. General nonlinear problems with linear constraints are sometimes solved as a sequence of quadratic program approximations. Many nonlinear relations occurring in nature are not quadratic, but can be approximated by quadratic functions and then solved with the methods just described.<br>
However, a wide variety of problems fall naturally into the form of quadratic programs. The kinetic energy of a projectile is a quadratic function of its velocity. The least-squares problem in regression analysis has been modeled as a quadratic program. Certain problems in production planning, econometrics, activation analysis in chemical mixture problems, and in financial portfolio management are often treated as quadratic problems. We will elaborate on this last problem in the following example.<br>

<ul />

**Example 5.3**

A classical problem that is often used to illustrate the use of the quadratic programming model is called **portfolio selection**. A portfolio is a collection of assets, all of which have positive present values (called prices) and which also have positive future values that are currently unknown. Analysts often use the term *rate of return on investment* to describe future value as follows:

$$\text{Future value } = \text{ Price } \times (1 + \text{ Rate of return on investment })$$

Future values are positive but certainly may be less than present values (prices).<br>
Rates of return are not known nor are they guaranteed. A very high expected return on an asset is usually accompanied by great variability. The future values can be estimated, but because such estimates are subject to error, there is a risk associated with any portfolio. The risk of a portfolio can be reduced by diversification, the extent of which is determined by the number of assets in the portfolio and the proportion of the total investment that is in each asset. It is generally easier to predict the future value of the portfolio than to predict the future values of the individual assets.<br>
The portfolio manager is responsible for assigning a weight to each asset held in the portfolio. The weight of the $i$-th asset is the ratio of the dollar amount invested in that asset, divided by the total dollar value of the portfolio. The sum of the weights must be one, and all weights are non-negative. A portfolio p is defined by this set of weights. We will see that these weights determine the portfolio’s expected future value as well as the portfolio’s risk.<br>
The portfolio manager generally begins his decision making process with 

<ul />

• A fixed amount of money to be invested.<br>
• A list of $n$ assets to invest in.<br>
• The expected return of each asset.<br>
• The variance of each asset return.<br>
• All covariances.<br>

</ul>

If risk were of no concern, the manager would undoubtedly just invest all the money in the one asset offering the greatest expected return, that is, assigning a weight of 1 to that asset and 0 to all the others, regardless of risk. But risk almost always is a consideration, and most investors are risk-averse.<br>
It is desirable to maximize return *and* minimize risk, but in a competitive market, prices fluctuate so that the *safer* investments are more expensive than the *riskier* ones. So, in general, it is not possible to simultaneously achieve both goals of maximizing return and minimizing risk. Instead, we define a class of **efficient portfolios**. A portfolio is said to be efficient if either 

<ul />

• There is no other less risky portfolio having as high a return.<br>
or<br>
• There is no other more profitable portfolio having as little risk.<br>

</ul>

Thus, the problem of efficient portfolio selection can be viewed as having primal and dual expressions: to minimize variance subject to a specified expected return, or to maximize expected return subject to a specified variance.<br>
Let $n$ be the number of assets being considered, and let $r_i$ be the expected return on the $i$-th asset. We will let $W$ denote a column vector of $n$ asset weights, indicating what fraction of the portfolio should be allocated to each asset. We use a variance-covariance matrix $V$ in which diagonal element $v_{ii}$ is the variance of the $i$-th asset, and the offdiagonal elements $v_{ij} = v_{ji}$ denote the covariance between the $i$-th and $j$-th assets. Then, *risk* is defined as the variance $σ^2$ of the portfolio p as:

$$\text{ risk } (p) = \sigma^2(\text{p}) = W^T VW$$

The expected return of the portfolio $p$ is given by:

$$E(\text{p}) = W^T R$$

where $R$ is a column vector of expected asset returns. So the portfolio selection problem can be expressed as

$$\text{minimize } z = W^T VW $$

$$\text{subject to } \sum_{i=1}^{n} w_i r_i \geq P $$

$$\sum_{i=1}^{n} w_i = 1$$

$$w_i \geq 0 \text{ for i = 1, ..., n}$$

where $P$ is a desired minimum return on investment.
Equivalently, we could

$$\text{maximize } z = W^T R$$

$$\text{subject to } W^T VW \leq Q$$

$$\sum_{i=1}^{n} w_i = 1$$

$$w_i \ge 0 \text{ for i = 1, ..., n}$$

where $Q$ is a desired maximum risk. The first of these formulations is clearly in the form of a quadratic programming problem with linear constraints. The noted economist Harry Markowitz (1959) is credited with formulating the portfolio selection problem as a quadratic programming model.

</ul>