#  **Karush–Kuhn–Tucker Conditions (Inequality Constraints)**

The most general nonlinear programming problem can be defined as 

$$
\begin{align*}
\text{maximize} \quad & f(x) \\
\text{subject to} \quad &  g_i(x) \leq 0 \text{ for } i=1, \dots, m \\  
\end{align*}
$$

where $x = (x_1, x_2, …, x_n)$. Clearly, *any* mathematical programming problem can be expressed in this form. It is tempting to introduce slack variables and convert all the inequality constraints into equalities, then apply the method of Lagrange multipliers. However, the $m$ extra variables introduce an unwelcome computational expense, and we have more attractive alternatives that we will now consider.<br>
Actually, we do try to extend the idea of Lagrange multipliers by recognizing that if the *unconstrained* optimum of $f(x)$ does not satisfy all the inequality constraints indicated earlier, then when the constraints are imposed, at least one of the constraints will be satisfied as an *equality*. That is, the constrained optimum will occur on a boundary of the feasible region.<br>
This observation suggests an algorithm for solving the problem. We begin by solving the unconstrained problem of maximizing $f(x)$. If this solution satisfies the constraints, stop. Otherwise, we repeatedly impose increasingly larger subsets of constraints (converted to equalities) until either a feasible solution is found via the method of Lagrange multipliers, or until it is determined that no feasible solution exists.<br>
Unfortunately, this method is very computationally demanding (and consequently essentially useless on most problems of practical size), as well as not guaranteeing that a solution found is globally optimal. Still, the Lagrange multiplier idea leads to what are known as the Karush–Kuhn–Tucker conditions that are necessary at a stationary point, corresponding to $x$ and $λ$, of a maximization problem. The Karush–Kuhn–Tucker conditions can be stated as: 

$$\frac{\delta f}{\delta x_j} - \sum_{i=1}^{m} \frac{\lambda_i g_i}{\delta x_j} = 0 \text{ for j=1, ..., n}$$

$$g_i(x) \leq 0 \text{ for i=1, ...,m}$$

$$\lambda_i \geq 0 \text{ for i=1, ..., m}$$

$$\lambda_i g_i(x) = 0 \text{ for i=1, ..., m}$$

The Karush–Kuhn–Tucker conditions correspond to the optimality conditions for linear programming where the $λ$’s represent the dual variables. The gradient of the objective function at the optimal solution, $x$, can be written as a non-negative linear combination of the gradients (normal vectors) of the active constraints. The second condition states that $x$ must be feasible. The third condition is non-negativity, and the fourth condition corresponds to complementary slackness: $λ$ can be positive only if the corresponding constraint is active $(g_i(x) = 0)$. If the $i$-th constraint is satisfied as a strict inequality, then the $i$-th resource is not scarce and there is no marginal value associated with having more of that resource. This is indicated by $λ_i = 0$.<br>
The Karush–Kuhn–Tucker necessary conditions are also sufficient for a maximization problem if the objective function $f(x)$ is concave and the feasible region is convex. Establishing the convexity and concavity and applying the Karush–Kuhn–Tucker necessary conditions do not yield procedures that are reasonable for direct practical numerical application. However, the Karush–Kuhn–Tucker conditions do form the very foundation of the theory of general mathematical programming, and will be seen again in the next section where—at last—we will see some *efficient* computational methods.
