#  **Lagrange Multipliers (Equality Constraints)**

The method of Lagrange multipliers is named after the 18th century French mathematician Joseph-Louis Lagrange, and applies to nonlinear optimization problems with equality constraints, which can be expressed in the form: 


$$
\begin{align*}
\text{maximize} \quad & f(x) \\
\text{subject to} \quad &  g_i(x) = b_i \text{for } i=1, \dots, m \\
\end{align*}
$$

where $x = (x_1, x_2, …, x_n)$.<br>
We wish to find a solution such that each $g_i(x) = b_i$, so we are going to rewrite the original problem as: 

$$\text{maximize } F(x,\lambda) = f(x) - \sum_{i=1}^{m} \lambda_i (g_i(x) - b_i)$$

The quantities $λ_i$ are called Lagrange multipliers, and it is clear that if all the equality constraints are met precisely, then $F(x, λ) = f(x)$ for any values of $λ_1, λ_2, …, λ_m$. We wish to find values of $λ_1, λ_2, …, λ_m$ and $x_1, x_2, …, x_n$ that maximize $F(x, λ)$ and also satisfy $g_i(x) = b_i$ for $i = 1, …, m$. Such a solution would solve our original equality constrained problem.<br>
We already know that a necessary condition for an optimum of $F(x, λ)$ is that $δF/δx_j = 0$ for $j = 1, …, n$ and $δF/δλ_i = 0$ for $i = 1, …, m$. Taking $(m + n)$ partial derivatives of $F$, with respect to the components of $x_j$ and the $λ_i$, and setting each equal to zero, we can write the necessary conditions as

$$\frac{\delta f(x)}{\delta x_j} - \sum_{i=1}^{m} \left[\lambda_i \frac{\delta g_i}{\delta x_j}\right] = 0 \text{for } j=1, \dots, n$$

$$g_i(x) - b_i = 0 \text{for } i=1, \dots, m$$

We now have a set of $(m + n)$ equations in $(m + n)$ variables, which may be solvable by some iterative technique such as Newton–Raphson. There may be more than one critical point, but if so, the global optimum will be among them.<br>
As a final observation, it is interesting to apply the method of Lagrange multipliers to the standard linear programming problem with constraints expressed as $Ax = b$, and to see that the Lagrange multipliers are precisely equivalent to the dual variables. This is merely a special case of a further generalization which will be examined next.