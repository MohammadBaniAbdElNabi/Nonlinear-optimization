#   **Quasi-Newton Methods**

The computational demands of repeatedly inverting the $n × n$ Hessian matrix $H$ motivated the development of a large number of modifications to the original Newton’s method. These modifications differ from one another primarily in the way that the second derivatives are approximated from one iteration to the next.<br>
These Quasi-Newton methods begin with an arbitrary negative definite approximation to $H$, or its inverse, and through a succession of improvements, eventually converge to the true matrix $H$. For example, the methods could begin with $H = –I$, a negative identity matrix at some initial point, $x^0$. The Newton direction corresponds to a simple gradient direction. We first perform a line search to get a new point, $x^1$. Then, based on the new point and the function value, we perform a rank 1 update to the matrix $H$ (and $H^{–1}$) which fits the current points with a quadratic. In so doing, we correct the estimate of $H$ in one dimension, but we also maintain a negative definite approximation. This process is repeated using the new estimate of $H$ to perform a line search and get a new maximum at $x^2$. After $n$ iterations on a negative definite quadratic function, the approximation is exact.<br>
The first such method was introduced by Davidon (1959), and shortly thereafter was improved upon by Fletcher and Powell (1963). The combined technique was known as the DFP method. A few years later, minor variations were proposed independently by Broyden (1970), Fletcher (1970), Goldfarb (1969), and Shanno (1970) and these became known collectively as the BFGS update formula. This is the method upon which almost all commercial software for nonlinear unconstrained optimization is based. The mathematical foundations and the precise formula typically used for updating the Hessian matrix is given in Beale (1959) and Avriel (1976).