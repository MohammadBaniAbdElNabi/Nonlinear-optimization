# **One-Dimensional Search**

The process begins by establishing an upper limit $x_u$ and a lower limit $x_l$, within which an optimum is known to exist, and choosing an initial trial solution $x$ to be halfway between the bounds: 

$$
x = \frac{x_u + x_l}{2}
$$

Suppose a function $f(x)$ is to be maximized and that $f(x)$ is concave between $x_u$ and $x_l$. Then the general idea is to examine the slope of $f(x)$ at the current trial solution $x$. If the slope is positive, then $f(x)$ is increasing and the optimum $x*$ is greater than $x$, so $x$ is a new lower bound on the set of trial solutions to be examined. If the slope is negative, then $f(x)$ is decreasing and the optimum $x*$ is less than $x$, so $x$ is a new upper bound. Each time a new bound is established, a new trial solution is computed (and choosing the midpoint is but one of several sensible rules). The sequence of trial solutions thus generated converges to the maximum at $x*$. In practice, the process terminates when the bounds $x_u$ and $x_l$ enclose an interval of some predetermined size $Îµ$, denoting an error tolerance. The algorithm can be stated succinctly as follows.